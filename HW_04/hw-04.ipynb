{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from getnewspaper import getNewsPaper\n",
    "#from getnewspaper import cosineSimilarity\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df_input):\n",
    "    df_output = df_input.copy()\n",
    "    df_output = df_output[~(df_output['description'].isna())]\n",
    "\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\", header=0,  delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv', header=0,  delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_other = pd.read_csv(\"other.csv\", header=0,  delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 170236 entries, 0 to 170235\n",
      "Data columns (total 3 columns):\n",
      "id             170236 non-null object\n",
      "name           170179 non-null object\n",
      "description    170179 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170236, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_train[df_train['description'].isna()]\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = transform_data(df_train)\n",
    "df_train = df_train[~(df_train['target'].isna())]\n",
    "\n",
    "# df_test = transform_data(df_test)\n",
    "df_test = df_test[~(df_test['description'].isna())]\n",
    "\n",
    "#df_other = transform_data(df_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Заведующий отделом/секцией в магазин YORK (Уру...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;В НОВЫЙ МАГАЗИН YORK (хозтовары) пр...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Наладчик станков и манипуляторов с ПУ</td>\n",
       "      <td>Обязанности:работа на токарных станках с ЧПУ T...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Разработчик С++ (Криптограф)</td>\n",
       "      <td>&lt;strong&gt;Требования:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Опыт про...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Фрезеровщик</td>\n",
       "      <td>&lt;p&gt;Условия:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;На работу вахтовым ме...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Мерчендайзер/продавец-консультант</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;Компания Палладиум Стандарт - призн...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                               name  \\\n",
       "0  0  Заведующий отделом/секцией в магазин YORK (Уру...   \n",
       "1  1              Наладчик станков и манипуляторов с ПУ   \n",
       "2  2                       Разработчик С++ (Криптограф)   \n",
       "3  3                                        Фрезеровщик   \n",
       "4  4                  Мерчендайзер/продавец-консультант   \n",
       "\n",
       "                                         description  target  \n",
       "0  <p><strong>В НОВЫЙ МАГАЗИН YORK (хозтовары) пр...     1.0  \n",
       "1  Обязанности:работа на токарных станках с ЧПУ T...     0.0  \n",
       "2  <strong>Требования:</strong> <ul> <li>Опыт про...     0.0  \n",
       "3  <p>Условия:</p> <ul> <li>На работу вахтовым ме...     0.0  \n",
       "4  <p><strong>Компания Палладиум Стандарт - призн...     1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_wordlist(text):\n",
    "    text = re.sub('n\\'t', ' not', text)\n",
    "    \n",
    "    text = re.sub('[^а-яА-Яa-zA-Z]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "\n",
    "    #stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "def text_to_sentences(text):\n",
    "    if pd.isnull(text):\n",
    "        text = ''\n",
    "        \n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "        \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(text_to_wordlist(raw_sentence))\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skulivec\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec \n",
    "def learn_W2V_model():\n",
    "    %time\n",
    "    sentences = []\n",
    "    \n",
    "    for review in df_train[\"description\"]:\n",
    "        sentences += text_to_sentences(review)\n",
    "        \n",
    "    %time\n",
    "    for review in df_other[\"description\"]:\n",
    "        sentences += text_to_sentences(review)\n",
    "    \n",
    "    %time\n",
    "    for review in df_test[\"description\"]:\n",
    "        sentences += text_to_sentences(review)\n",
    "        \n",
    "    # список параметров, которые можно менять по вашему желанию\n",
    "    num_features = 300  # итоговая размерность вектора каждого слова\n",
    "    min_word_count = 5  # минимальная частотность слова, чтобы оно попало в модель\n",
    "    num_workers = 56     # количество ядер вашего процессора, чтоб запустить обучение в несколько потоков\n",
    "    context = 10        # размер окна \n",
    "    downsampling = 1e-3 # внутренняя метрика модели\n",
    "    \n",
    "    model = Word2Vec(sentences, workers=num_workers, size=num_features,\n",
    "                     min_count=min_word_count, window=context, sample=downsampling)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "model = learn_W2V_model()\n",
    "model0 = model\n",
    "model.init_sims(replace=True)\n",
    "model.wv.save_word2vec_format('model_save_word2vec_format.vec', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "num_features = 300\n",
    "#model = KeyedVectors.load_word2vec_format('model_save_word2vec_format.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_to_vec(words, model, size):\n",
    "    text_vec = np.zeros((size,), dtype=\"float32\")\n",
    "    n_words = 0\n",
    "\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words = n_words + 1\n",
    "            text_vec = np.add(text_vec, model[word])\n",
    "    \n",
    "    if n_words != 0:\n",
    "        text_vec /= n_words\n",
    "    return text_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_vecs(texts, model, size):\n",
    "    texts_vecs = np.zeros((len(texts), size), dtype=\"float32\")\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        texts_vecs[i] = text_to_vec(text, model, size)\n",
    "\n",
    "    return texts_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем для всех текстов из train\n",
    "train_like_word_list = [sum(text_to_sentences(text), []) for text in df_train['description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = texts_to_vecs(train_like_word_list, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем тоже самое для test\n",
    "test_like_word_list = [sum(text_to_sentences(text), []) for text in df_test['description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vecs = texts_to_vecs(test_like_word_list, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(train_like_word_list, open( \"train_like_word_list.dat\", \"wb\" ) )\n",
    "#train_like_word_list = pickle.load( open( \"train_like_word_list.dat\", \"rb\" ) )\n",
    "\n",
    "pickle.dump(train_vecs, open( \"train_vecs.dat\", \"wb\" ) )\n",
    "#train_vecs = pickle.load( open( \"train_vecs.dat\", \"rb\" ) )\n",
    "\n",
    "pickle.dump(test_like_word_list, open( \"test_like_word_list.dat\", \"wb\" ) )\n",
    "#test_like_word_list = pickle.load( open( \"test_like_word_list.dat\", \"rb\" ) )\n",
    "\n",
    "pickle.dump(test_vecs, open( \"test_vecs.dat\", \"wb\" ) )\n",
    "#test_vecs = pickle.load( open( \"test_vecs.dat\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((170179, 300), (170179, 3))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vecs.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "test_size=0.3\n",
    "random_state=123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = train_vecs\n",
    "y = df_train[\"target\"]\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    #'max_depth': randint(4, 8),\n",
    "    #'min_samples_leaf': randint(4, 8),\n",
    "    'max_features': ['auto', 'log2', None],\n",
    "    'class_weight': [None, 'balanced']}\n",
    "cv = StratifiedKFold(n_splits=5, random_state=random_state, shuffle=True)\n",
    "\n",
    "forest = RandomForestClassifier(random_state=random_state, n_estimators=150, n_jobs=-1)\n",
    "random_search = RandomizedSearchCV(forest, param_distributions=param_grid, n_iter=12, n_jobs=-1,\n",
    "                                   cv=cv, scoring='roc_auc', random_state=random_state) # , 'precision', 'f1', 'recall', 'roc_auc'\n",
    "random_search.fit(X, y)\n",
    "print(random_search.scoring)\n",
    "print('random_search.best_params_')\n",
    "print(random_search.best_params_)\n",
    "print('random_search.best_score_')\n",
    "print(random_search.best_score_)\n",
    "print('random_search.best_estimator_')\n",
    "print(random_search.best_estimator_)\n",
    "\n",
    "\n",
    "\n",
    "RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, \n",
    "                       min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                       max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                       min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, \n",
    "                       random_state=None, verbose=0, warm_start=False, class_weight=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X, X_validate, y, y_validate = train_test_split(train_vecs, df_train[\"target\"], test_size=test_size, random_state=random_state)\n",
    "\n",
    "forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=-1,\n",
    "            oob_score=False, random_state=123, verbose=0, warm_start=False)\n",
    "\n",
    "forest = forest.fit(X, y)\n",
    "predict = forest.predict(X_validate)\n",
    "vals = forest.predict_proba(X_validate)[:,1]\n",
    "print('Roc', roc_auc_score(y_validate, vals)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Воспользуемся train_vecs, test_vecs, train[\"target\"] \n",
    "    как матрица фичей обучающей выборки, матрица фичей тестовой выборки, таргет для обучающей выборки соответственно\n",
    "\n",
    " Стандартный случайный лес в таком случае\n",
    "\n",
    "\n",
    "forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=-1,\n",
    "            oob_score=False, random_state=123, verbose=0, warm_start=False)\n",
    "forest = forest.fit(train_vecs, df_train[\"target\"])\n",
    "predict = forest.predict(test_vecs)\n",
    "\n",
    "\n",
    "vals=forest.predict_proba(test_vecs)[:,1]\n",
    "pd.DataFrame({'id':df_test['id'], 'target':[i for i in vals]}).to_csv('sampleSubmission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "roc_auc\n",
    "random_search.best_params_\n",
    "{'max_features': 'auto', 'criterion': 'entropy', 'class_weight': None}\n",
    "random_search.best_score_\n",
    "0.9904878646416567\n",
    "random_search.best_estimator_\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=-1,\n",
    "            oob_score=False, random_state=123, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X, X_validate, y, y_validate = train_test_split(train_vecs, df_train[\"target\"], test_size=test_size, random_state=random_state)\n",
    "\n",
    "\n",
    "model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=1, missing=None, n_estimators=648,\n",
    "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1)\n",
    "#XGBClassifier(n_estimators=1000)\n",
    "#XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       #colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "       #max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
    "       #n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
    "       #reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       #silent=True, subsample=1)\n",
    "\n",
    "model.fit(X, y)\n",
    "#model.fit(X, y, \n",
    "#eval_set=[(X, y), (X_validate, y_validate)], \n",
    "#eval_metric='auc', \n",
    "#early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[648]\tvalidation_0-auc:0.993499\tvalidation_1-auc:0.988759\n",
    "\n",
    "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
    "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#y_pred = model.predict_proba(X_validate)\n",
    "y_pred = model.predict_proba(X_validate)[:,1]\n",
    "#predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('Roc', roc_auc_score(y_validate, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roc 0.9571608342096216"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "param = {'booster' : \"gbtree\", \n",
    "                            objective = \"binary:logistic\",\n",
    "                            eval_metric = 'auc', \n",
    "                            max_depth = sample(5:8, 1),\n",
    "                            eta = runif(1, 0.01, 0.3),\n",
    "                            gamma = runif(1, 0.15, 0.2),\n",
    "                            subsample = runif(1, 0.5, 1),\n",
    "                            colsample_bytree = runif(1, .5, 1),\n",
    "                            min_child_weight = sample(1:10, 1),\n",
    "                            num_parallel_tree = sample(1:4, 1)\n",
    "                            max_delta_step = sample(1:10, 1)\n",
    "        }\n",
    "\n",
    "xgboost.cv(params, \n",
    "dtrain, \n",
    "num_boost_round=10, \n",
    "nfold=3, \n",
    "stratified=False, \n",
    "folds=None, \n",
    "metrics=(), \n",
    "obj=None, \n",
    "feval=None, \n",
    "maximize=False, \n",
    "early_stopping_rounds=None, \n",
    "fpreproc=None, \n",
    "as_pandas=True, \n",
    "verbose_eval=None, \n",
    "show_stdv=True, \n",
    "seed=0, \n",
    "callbacks=None, \n",
    "shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_vecs, df_train[\"target\"], test_size=test_size, random_state=random_state)\n",
    "#X_train, y_train = train_vecs, df_train[\"target\"]\n",
    "#X_valid, y_valid = test_vecs, df_test[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=-1,\n",
    "            oob_score=False, random_state=123, verbose=0, warm_start=False),\n",
    "    XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=1, missing=None, n_estimators=648,\n",
    "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1),\n",
    "    Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression())\n",
    "    ])    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_score_cust(y_true, y_hat):\n",
    "    return roc_auc_score(y_true, y_hat[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vecstack import stacking\n",
    "S_train, S_valid = stacking(models,\n",
    "                               X_train, y_train, X_valid,\n",
    "                               regression=False,\n",
    "                               mode='oof_pred_bag', \n",
    "                               needs_proba=True,\n",
    "                               metric=roc_auc_score_cust,\n",
    "                               n_folds=5,                \n",
    "                               stratified=True,          \n",
    "                               shuffle=True,             \n",
    "                               random_state=123,         \n",
    "                               verbose=2)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_model = GradientBoostingClassifier(n_estimators=300, max_depth=3,\n",
    "                                       learning_rate=0.01, \n",
    "                                       random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_model.fit(S_train, y_train)\n",
    "y_hat = last_model.predict_proba(S_valid)\n",
    "roc_auc_score(y_valid, y_hat[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.9906724733360112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_hat = model.predict_proba(X_valid)\n",
    "    score = roc_auc_score(y_valid, y_hat[:, 1])\n",
    "    print('{}: {}'.format(model.__str__().split('(')[0], score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier: 0.9902368802467469\n",
    "XGBClassifier: 0.9889985218951911\n",
    "Pipeline: 0.9811719514839687"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_vecs, df_train[\"target\"]\n",
    "X_valid = test_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_train, S_valid = stacking(models,\n",
    "                               X_train, y_train, X_valid,\n",
    "                               regression=False,\n",
    "                               mode='oof_pred_bag', \n",
    "                               needs_proba=True,\n",
    "                               metric=roc_auc_score_cust,\n",
    "                               n_folds=5,                \n",
    "                               stratified=True,          \n",
    "                               shuffle=True,             \n",
    "                               random_state=123,         \n",
    "                               verbose=2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_model = GradientBoostingClassifier(n_estimators=300, max_depth=3,\n",
    "                                       learning_rate=0.01, \n",
    "                                       random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_model.fit(S_train, y_train)\n",
    "y_hat = last_model.predict_proba(S_valid)\n",
    "#roc_auc_score(y_valid, y_hat[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task:         [classification]\n",
    "n_classes:    [2]\n",
    "metric:       [roc_auc_score_cust]\n",
    "mode:         [oof_pred_bag]\n",
    "n_models:     [3]\n",
    "\n",
    "model  0:     [RandomForestClassifier]\n",
    "    fold  0:  [0.99047541]\n",
    "    fold  1:  [0.98994370]\n",
    "    fold  2:  [0.99085160]\n",
    "    fold  3:  [0.99031660]\n",
    "    fold  4:  [0.99068257]\n",
    "    ----\n",
    "    MEAN:     [0.99045398] + [0.00031307]\n",
    "    FULL:     [0.99045241]\n",
    "\n",
    "model  1:     [XGBClassifier]\n",
    "    fold  0:  [0.98938222]\n",
    "    fold  1:  [0.98815088]\n",
    "    fold  2:  [0.98961011]\n",
    "    fold  3:  [0.98902734]\n",
    "    fold  4:  [0.98946072]\n",
    "    ----\n",
    "    MEAN:     [0.98912625] + [0.00052396]\n",
    "    FULL:     [0.98912563]\n",
    "\n",
    "model  2:     [Pipeline]\n",
    "    fold  0:  [0.98107841]\n",
    "    fold  1:  [0.97973473]\n",
    "    fold  2:  [0.98188632]\n",
    "    fold  3:  [0.98060899]\n",
    "    fold  4:  [0.98158286]\n",
    "    ----\n",
    "    MEAN:     [0.98097826] + [0.00075932]\n",
    "    FULL:     [0.98097408]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals=y_hat[:,1]\n",
    "\n",
    "vals = vals[~(df_test['description'].isna())]\n",
    "df_test = df_test[~(df_test['description'].isna())]\n",
    "\n",
    "pd.DataFrame({'id':df_test['id'], 'target':[i for i in vals]}).to_csv('sampleSubmission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
